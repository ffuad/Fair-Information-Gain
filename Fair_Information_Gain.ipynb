{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m66xEDhJ8gHq",
        "outputId": "11c677ec-88d2-4153-b778-120323061e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/3.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install ucimlrepo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "blUSg2LBEEqf"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "adult = fetch_ucirepo(id=2)\n",
        "\n",
        "def load_data():\n",
        "    X = adult.data.features\n",
        "    y = adult.data.targets\n",
        "    for col in X.select_dtypes(include=\"object\").columns:\n",
        "      X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
        "    for col in y.select_dtypes(include=\"object\").columns:\n",
        "      y[col] = pd.factorize(y[col])[0]  # Encode categorical variables as integers\n",
        "    return X, y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict, deque"
      ],
      "metadata": {
        "id": "bxc5I590Umdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hoeffding Tree with Gini Index"
      ],
      "metadata": {
        "id": "jOi1oqL9cNY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HoeffdingTreeNode:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None, is_leaf=True):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.right = right\n",
        "        self.is_leaf = is_leaf\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.data_points = []  # Store data points until a split\n"
      ],
      "metadata": {
        "id": "NwJE3t0CUvLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HoeffdingTree:\n",
        "    def __init__(self, delta=0.01):\n",
        "        self.root = HoeffdingTreeNode()\n",
        "        self.delta = delta\n",
        "        self.n = 0  # Number of samples seen\n",
        "\n",
        "    def hoeffding_bound(self, R, n):\n",
        "        return np.sqrt((R**2 * np.log(1 / self.delta)) / (2 * n))\n",
        "\n",
        "    def update(self, x, y):\n",
        "        self.n += 1\n",
        "        node = self.root\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        node.class_counts[y] += 1\n",
        "        node.data_points.append((x, y))\n",
        "\n",
        "        if self.n % 100 == 0:  # Check for split every 100 samples\n",
        "            self.attempt_split(node)\n",
        "\n",
        "    def attempt_split(self, node):\n",
        "        best_feature, best_threshold = None, None\n",
        "        best_gini = float('inf')\n",
        "\n",
        "        # Loop over features\n",
        "        for feature in range(len(node.data_points[0][0])):  # Number of features\n",
        "            feature_values = [x[feature] for x, _ in node.data_points]\n",
        "            thresholds = np.unique(feature_values)\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_counts = defaultdict(int)\n",
        "                right_counts = defaultdict(int)\n",
        "\n",
        "                for x, y in node.data_points:\n",
        "                    if x[feature] < threshold:\n",
        "                        left_counts[y] += 1\n",
        "                    else:\n",
        "                        right_counts[y] += 1\n",
        "\n",
        "                gini = self.calculate_gini(left_counts, right_counts)\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # Perform split if a suitable feature and threshold were found\n",
        "        if best_feature is not None:\n",
        "            node.feature = best_feature\n",
        "            node.threshold = best_threshold\n",
        "            node.is_leaf = False\n",
        "            node.left = HoeffdingTreeNode()\n",
        "            node.right = HoeffdingTreeNode()\n",
        "\n",
        "            # Distribute data points to child nodes\n",
        "            for x, y in node.data_points:\n",
        "                if x[best_feature] < best_threshold:\n",
        "                    node.left.class_counts[y] += 1\n",
        "                    node.left.data_points.append((x, y))\n",
        "                else:\n",
        "                    node.right.class_counts[y] += 1\n",
        "                    node.right.data_points.append((x, y))\n",
        "\n",
        "            node.data_points = []  # Clear data points after splitting\n",
        "\n",
        "    def calculate_gini(self, left_counts, right_counts):\n",
        "        total_left = sum(left_counts.values())\n",
        "        total_right = sum(right_counts.values())\n",
        "        total = total_left + total_right\n",
        "        gini_left = 1.0 - sum((count / total_left) ** 2 for count in left_counts.values()) if total_left > 0 else 0\n",
        "        gini_right = 1.0 - sum((count / total_right) ** 2 for count in right_counts.values()) if total_right > 0 else 0\n",
        "        return (total_left / total) * gini_left + (total_right / total) * gini_right\n",
        "\n",
        "    def predict(self, x):\n",
        "        node = self.root\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        # If node.class_counts is empty, return a default class (e.g., majority class in the dataset)\n",
        "        if node.class_counts:\n",
        "            return max(node.class_counts, key=node.class_counts.get)\n",
        "        else:\n",
        "            # Default class if no counts in this leaf (returning the most frequent class seen globally)\n",
        "            return max(self.root.class_counts, key=self.root.class_counts.get)\n",
        "\n"
      ],
      "metadata": {
        "id": "jDlapcHMUw9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_data()  # Replace with actual data loading function\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "df = pd.concat([X_train, y_train], axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m72z2n4fWOtg",
        "outputId": "c5eba1e9-98b2-4f0c-e16e-1a75eaeaa2c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[col] = pd.factorize(X[col])[0]  # Encode categorical variables as integers\n",
            "<ipython-input-2-6d554f8915d0>:11: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y[col] = pd.factorize(y[col])[0]  # Encode categorical variables as integers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tree = HoeffdingTree()\n",
        "for index, row in df.iterrows():\n",
        "    x1 = row[:-1].values  # Features\n",
        "    y1 = row.iloc[-1]     # Label\n",
        "    tree.update(x1, y1)"
      ],
      "metadata": {
        "id": "GfJlhsz1U03r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on new data\n",
        "y_pred = []\n",
        "for index, row in X_test.iterrows():\n",
        "    x1 = row.values  # Features\n",
        "    prediction = tree.predict(x1)\n",
        "    y_pred.append(prediction)\n",
        "    # print(f'Prediction: {prediction}')\n"
      ],
      "metadata": {
        "id": "NVnvDXAQW1Tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_act = y_test.values.flatten()\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Assuming y_pred and y_act are your predicted and actual labels\n",
        "accuracy = accuracy_score(y_act, y_pred)\n",
        "f1 = f1_score(y_act, y_pred, average='macro')  # Use 'micro', 'macro', or 'weighted' based on your case\n",
        "precision = precision_score(y_act, y_pred, average='macro')\n",
        "recall = recall_score(y_act, y_pred, average='macro')\n",
        "cm = confusion_matrix(y_act, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score (Macro): {f1:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRdDwNaIXDDT",
        "outputId": "cc693543-6bef-4cf1-a94f-0644d16cb8a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5784\n",
            "F1 Score (Macro): 0.3690\n",
            "Precision (Macro): 0.4928\n",
            "Recall (Macro): 0.4054\n",
            "Confusion Matrix:\n",
            "[[4465  328  125   18]\n",
            " [ 591  929    7   35]\n",
            " [2063  171  234   10]\n",
            " [ 303  455   13   22]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ehOcwl8vPM-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hoeffding Tree with only Information Gain"
      ],
      "metadata": {
        "id": "TT18IikubmXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class HoeffdingTreeNode:\n",
        "    def __init__(self):\n",
        "        self.is_leaf = True\n",
        "        self.feature = None\n",
        "        self.threshold = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.data_points = []\n",
        "\n",
        "class HoeffdingTree:\n",
        "    def __init__(self, delta=0.01):\n",
        "        self.root = HoeffdingTreeNode()\n",
        "        self.delta = delta\n",
        "        self.n = 0  # Number of samples seen\n",
        "\n",
        "    def hoeffding_bound(self, R, n):\n",
        "        return np.sqrt((R**2 * np.log(1 / self.delta)) / (2 * n))\n",
        "\n",
        "    def update(self, x, y):\n",
        "        self.n += 1\n",
        "        node = self.root\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        node.class_counts[y] += 1\n",
        "        node.data_points.append((x, y))\n",
        "\n",
        "        if self.n % 100 == 0:  # Check for split every 100 samples\n",
        "            self.attempt_split(node)\n",
        "\n",
        "    def calculate_entropy(self, class_counts):\n",
        "        total = sum(class_counts.values())\n",
        "        return -sum((count / total) * np.log2(count / total) for count in class_counts.values() if count > 0)\n",
        "\n",
        "    def calculate_information_gain(self, left_counts, right_counts):\n",
        "        # Total counts before split\n",
        "        total_counts = defaultdict(int, left_counts)\n",
        "        for k, v in right_counts.items():\n",
        "            total_counts[k] += v\n",
        "\n",
        "        # Calculate entropy before split\n",
        "        total_entropy = self.calculate_entropy(total_counts)\n",
        "\n",
        "        # Entropy for left and right child nodes\n",
        "        total_left = sum(left_counts.values())\n",
        "        total_right = sum(right_counts.values())\n",
        "        total = total_left + total_right\n",
        "\n",
        "        entropy_left = self.calculate_entropy(left_counts) if total_left > 0 else 0\n",
        "        entropy_right = self.calculate_entropy(right_counts) if total_right > 0 else 0\n",
        "\n",
        "        # Weighted average of entropies\n",
        "        entropy_after_split = (total_left / total) * entropy_left + (total_right / total) * entropy_right\n",
        "\n",
        "        # Information Gain\n",
        "        return total_entropy - entropy_after_split\n",
        "\n",
        "    def attempt_split(self, node):\n",
        "        best_feature, best_threshold = None, None\n",
        "        best_info_gain = -float('inf')  # Set to negative infinity to find maximum information gain\n",
        "\n",
        "        # Loop over features\n",
        "        for feature in range(len(node.data_points[0][0])):  # Number of features\n",
        "            feature_values = [x[feature] for x, _ in node.data_points]\n",
        "            thresholds = np.unique(feature_values)\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_counts = defaultdict(int)\n",
        "                right_counts = defaultdict(int)\n",
        "\n",
        "                for x, y in node.data_points:\n",
        "                    if x[feature] < threshold:\n",
        "                        left_counts[y] += 1\n",
        "                    else:\n",
        "                        right_counts[y] += 1\n",
        "\n",
        "                # Calculate information gain\n",
        "                info_gain = self.calculate_information_gain(left_counts, right_counts)\n",
        "                if info_gain > best_info_gain:\n",
        "                    best_info_gain = info_gain\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        # Perform split if a suitable feature and threshold were found\n",
        "        if best_feature is not None:\n",
        "            node.feature = best_feature\n",
        "            node.threshold = best_threshold\n",
        "            node.is_leaf = False\n",
        "            node.left = HoeffdingTreeNode()\n",
        "            node.right = HoeffdingTreeNode()\n",
        "\n",
        "            # Distribute data points to child nodes\n",
        "            for x, y in node.data_points:\n",
        "                if x[best_feature] < best_threshold:\n",
        "                    node.left.class_counts[y] += 1\n",
        "                    node.left.data_points.append((x, y))\n",
        "                else:\n",
        "                    node.right.class_counts[y] += 1\n",
        "                    node.right.data_points.append((x, y))\n",
        "\n",
        "            node.data_points = []  # Clear data points after splitting\n",
        "\n",
        "    def predict(self, x):\n",
        "        node = self.root\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        # If node.class_counts is empty, return a default class (e.g., majority class in the dataset)\n",
        "        if node.class_counts:\n",
        "            return max(node.class_counts, key=node.class_counts.get)\n",
        "        else:\n",
        "            # Default class if no counts in this leaf (returning the most frequent class seen globally)\n",
        "            return max(self.root.class_counts, key=self.root.class_counts.get)\n"
      ],
      "metadata": {
        "id": "-rdPDi_hPNl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage example\n",
        "tree = HoeffdingTree()\n",
        "for index, row in df.iterrows():\n",
        "    x1 = row[:-1].values  # Features\n",
        "    y1 = row.iloc[-1]     # Label\n",
        "    tree.update(x1, y1)\n",
        "\n",
        "# Predict on new data\n",
        "y_pred = []\n",
        "for index, row in X_test.iterrows():\n",
        "    x1 = row.values  # Features\n",
        "    prediction = tree.predict(x1)\n",
        "    y_pred.append(prediction)\n"
      ],
      "metadata": {
        "id": "_owjkB1pPplH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_act = y_test.values.flatten()\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Assuming y_pred and y_act are your predicted and actual labels\n",
        "accuracy = accuracy_score(y_act, y_pred)\n",
        "f1 = f1_score(y_act, y_pred, average='macro')  # Use 'micro', 'macro', or 'weighted' based on your case\n",
        "precision = precision_score(y_act, y_pred, average='macro')\n",
        "recall = recall_score(y_act, y_pred, average='macro')\n",
        "cm = confusion_matrix(y_act, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score (Macro): {f1:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVIt7cGmVrda",
        "outputId": "55192c7b-2e75-43a7-a89f-a1a161d56e86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5830\n",
            "F1 Score (Macro): 0.3632\n",
            "Precision (Macro): 0.5606\n",
            "Recall (Macro): 0.4015\n",
            "Confusion Matrix:\n",
            "[[4575  288   70    3]\n",
            " [ 646  902    4   10]\n",
            " [2136  131  202    9]\n",
            " [ 331  439    7   16]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8cSn5LjWVubU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hoeffding Tree with Fair Information Gain"
      ],
      "metadata": {
        "id": "s1f1S9lVb84s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class FairHoeffdingTreeNode:\n",
        "    def __init__(self):\n",
        "        self.is_leaf = True\n",
        "        self.feature = None\n",
        "        self.threshold = None\n",
        "        self.left = None\n",
        "        self.right = None\n",
        "        self.class_counts = defaultdict(int)\n",
        "        self.data_points = []\n",
        "        self.sensitive_counts = defaultdict(lambda: defaultdict(int))  # Counts based on sensitive attribute\n",
        "\n",
        "class FairHoeffdingTree:\n",
        "    def __init__(self, delta=0.01):\n",
        "        self.root = FairHoeffdingTreeNode()\n",
        "        self.delta = delta\n",
        "        self.n = 0  # Number of samples seen\n",
        "\n",
        "    def hoeffding_bound(self, R, n):\n",
        "        return np.sqrt((R**2 * np.log(1 / self.delta)) / (2 * n))\n",
        "\n",
        "    def update(self, x, y, sensitive_value):\n",
        "        self.n += 1\n",
        "        node = self.root\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        node.class_counts[y] += 1\n",
        "        node.sensitive_counts[sensitive_value][y] += 1\n",
        "        node.data_points.append((x, y, sensitive_value))\n",
        "\n",
        "        if self.n % 100 == 0:  # Check for split every 100 samples\n",
        "            self.attempt_split(node)\n",
        "\n",
        "    def calculate_entropy(self, class_counts):\n",
        "        total = sum(class_counts.values())\n",
        "        return -sum((count / total) * np.log2(count / total) for count in class_counts.values() if count > 0)\n",
        "\n",
        "    def calculate_information_gain(self, left_counts, right_counts):\n",
        "        total_counts = defaultdict(int, left_counts)\n",
        "        for k, v in right_counts.items():\n",
        "            total_counts[k] += v\n",
        "\n",
        "        total_entropy = self.calculate_entropy(total_counts)\n",
        "        total_left = sum(left_counts.values())\n",
        "        total_right = sum(right_counts.values())\n",
        "        total = total_left + total_right\n",
        "\n",
        "        entropy_left = self.calculate_entropy(left_counts) if total_left > 0 else 0\n",
        "        entropy_right = self.calculate_entropy(right_counts) if total_right > 0 else 0\n",
        "\n",
        "        entropy_after_split = (total_left / total) * entropy_left + (total_right / total) * entropy_right\n",
        "        return total_entropy - entropy_after_split\n",
        "\n",
        "    def calculate_fairness_gain(self, left_sensitive_counts, right_sensitive_counts):\n",
        "        def discrimination_score(sensitive_counts):\n",
        "            # Retrieve counts with default 0 if key doesn't exist\n",
        "            DR = sensitive_counts['sensitive'].get(0, 0)  # Deprived-Rejected\n",
        "            DG = sensitive_counts['sensitive'].get(1, 0)  # Deprived-Granted\n",
        "            FR = sensitive_counts['non-sensitive'].get(0, 0)  # Favored-Rejected\n",
        "            FG = sensitive_counts['non-sensitive'].get(1, 0)  # Favored-Granted\n",
        "\n",
        "            # Handle division by zero in case of no samples\n",
        "            if (FG + FR) == 0 or (DG + DR) == 0:\n",
        "                return 0\n",
        "            return abs(FG / (FG + FR) - DG / (DG + DR))\n",
        "\n",
        "        # Calculate discrimination scores before and after the split\n",
        "        before_split = discrimination_score(left_sensitive_counts) + discrimination_score(right_sensitive_counts)\n",
        "        after_split = sum(\n",
        "            len(group) / self.n * discrimination_score(group)\n",
        "            for group in [left_sensitive_counts, right_sensitive_counts]\n",
        "        )\n",
        "\n",
        "        return before_split - after_split\n",
        "\n",
        "    def calculate_fair_information_gain(self, left_counts, right_counts, left_sensitive_counts, right_sensitive_counts):\n",
        "        IG = self.calculate_information_gain(left_counts, right_counts)\n",
        "        FG = self.calculate_fairness_gain(left_sensitive_counts, right_sensitive_counts)\n",
        "        return IG * FG if FG != 0 else IG\n",
        "\n",
        "    def attempt_split(self, node):\n",
        "        best_feature, best_threshold = None, None\n",
        "        best_fig = -float('inf')\n",
        "\n",
        "        for feature in range(len(node.data_points[0][0])):  # Number of features\n",
        "            feature_values = [x[feature] for x, _, _ in node.data_points]\n",
        "            thresholds = np.unique(feature_values)\n",
        "\n",
        "            for threshold in thresholds:\n",
        "                left_counts, right_counts = defaultdict(int), defaultdict(int)\n",
        "                left_sensitive_counts = defaultdict(lambda: defaultdict(int))\n",
        "                right_sensitive_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "                for x, y, sensitive_value in node.data_points:\n",
        "                    if x[feature] < threshold:\n",
        "                        left_counts[y] += 1\n",
        "                        left_sensitive_counts[sensitive_value][y] += 1\n",
        "                    else:\n",
        "                        right_counts[y] += 1\n",
        "                        right_sensitive_counts[sensitive_value][y] += 1\n",
        "\n",
        "                fig = self.calculate_fair_information_gain(\n",
        "                    left_counts, right_counts, left_sensitive_counts, right_sensitive_counts\n",
        "                )\n",
        "\n",
        "                if fig > best_fig:\n",
        "                    best_fig = fig\n",
        "                    best_feature = feature\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_feature is not None:\n",
        "            node.feature = best_feature\n",
        "            node.threshold = best_threshold\n",
        "            node.is_leaf = False\n",
        "            node.left = FairHoeffdingTreeNode()\n",
        "            node.right = FairHoeffdingTreeNode()\n",
        "\n",
        "            for x, y, sensitive_value in node.data_points:\n",
        "                if x[best_feature] < best_threshold:\n",
        "                    node.left.class_counts[y] += 1\n",
        "                    node.left.sensitive_counts[sensitive_value][y] += 1\n",
        "                    node.left.data_points.append((x, y, sensitive_value))\n",
        "                else:\n",
        "                    node.right.class_counts[y] += 1\n",
        "                    node.right.sensitive_counts[sensitive_value][y] += 1\n",
        "                    node.right.data_points.append((x, y, sensitive_value))\n",
        "\n",
        "            node.data_points = []\n",
        "\n",
        "    def predict(self, x):\n",
        "        node = self.root\n",
        "        while not node.is_leaf:\n",
        "            if x[node.feature] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "\n",
        "        if node.class_counts:\n",
        "            return max(node.class_counts, key=node.class_counts.get)\n",
        "        else:\n",
        "            return max(self.root.class_counts, key=self.root.class_counts.get)\n"
      ],
      "metadata": {
        "id": "awRapmD5ObjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fair_tree = FairHoeffdingTree(delta=0.01)\n",
        "\n",
        "# Loop through each row in the dataset and update the tree\n",
        "for index, row in df.iterrows():\n",
        "    features = row.drop(['income', 'sex']).values  # Drop target and sensitive columns to get features\n",
        "    target = row['income']  # The target label\n",
        "    sensitive_value = row['sex']  # Sensitive attribute value\n",
        "\n",
        "    # Update the tree with each instance\n",
        "    fair_tree.update(features, target, sensitive_value)\n"
      ],
      "metadata": {
        "id": "oUc3gn6bOz-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = []\n",
        "# print(X_test.columns)\n",
        "for index, row in X_test.iterrows():\n",
        "\n",
        "    x1 = row.drop(['sex']).values  # Features\n",
        "    prediction = fair_tree.predict(x1)\n",
        "    y_pred.append(prediction)\n",
        "    # print(prediction)\n"
      ],
      "metadata": {
        "id": "CkI91p4eQIFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_act = y_test.values.flatten()\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "\n",
        "# Assuming y_pred and y_act are your predicted and actual labels\n",
        "accuracy = accuracy_score(y_act, y_pred)\n",
        "f1 = f1_score(y_act, y_pred, average='macro')  # Use 'micro', 'macro', or 'weighted' based on your case\n",
        "precision = precision_score(y_act, y_pred, average='macro')\n",
        "recall = recall_score(y_act, y_pred, average='macro')\n",
        "cm = confusion_matrix(y_act, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"F1 Score (Macro): {f1:.4f}\")\n",
        "print(f\"Precision (Macro): {precision:.4f}\")\n",
        "print(f\"Recall (Macro): {recall:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VF7w2RiJXaHP",
        "outputId": "a2b18b49-93da-427c-f954-064d9d87ec28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5835\n",
            "F1 Score (Macro): 0.3637\n",
            "Precision (Macro): 0.5609\n",
            "Recall (Macro): 0.4023\n",
            "Confusion Matrix:\n",
            "[[4575  288   70    3]\n",
            " [ 641  907    4   10]\n",
            " [2136  131  202    9]\n",
            " [ 328  442    7   16]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}